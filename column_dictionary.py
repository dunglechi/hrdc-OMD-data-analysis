"""
Smart Column Dictionary
T·ª± ƒë·ªông ph√°t hi·ªán v√† qu·∫£n l√Ω √Ω nghƒ©a c√°c c·ªôt v·ªõi AI
"""

import pandas as pd
import google.generativeai as genai
from typing import Dict, Any, Optional
import json
import streamlit as st

class ColumnDictionary:
    """
    Qu·∫£n l√Ω √Ω nghƒ©a c√°c c·ªôt v·ªõi AI inference v√† user editing
    """
    
    def __init__(self, df: pd.DataFrame, model):
        """
        Initialize Column Dictionary
        
        Args:
            df: DataFrame c·∫ßn ph√¢n t√≠ch
            model: Gemini model instance
        """
        self.df = df
        self.model = model
        self.dictionary = {}  # {column_name: column_info}
    
    def auto_detect_meanings(self) -> Dict[str, Dict[str, Any]]:
        """
        S·ª≠ d·ª•ng Gemini AI ƒë·ªÉ t·ª± ƒë·ªông ƒëo√°n √Ω nghƒ©a t·∫•t c·∫£ c√°c c·ªôt
        OPTIMIZED: Batch processing - ph√¢n t√≠ch t·∫•t c·∫£ c·ªôt trong 1 l·∫ßn g·ªçi API
        
        Returns:
            Dict[column_name, column_info]
        """
        if not self.model:
            # Fallback: basic inference without AI
            return self._basic_inference()
        
        try:
            # Batch inference - all columns at once
            self.dictionary = self._batch_infer_all_columns()
        except Exception as e:
            print(f"Batch inference failed: {e}, falling back to basic inference")
            self.dictionary = self._basic_inference()
        
        return self.dictionary
    
    def _batch_infer_all_columns(self) -> Dict[str, Dict[str, Any]]:
        """
        Ph√¢n t√≠ch T·∫§T C·∫¢ c√°c c·ªôt trong 1 l·∫ßn g·ªçi API (nhanh h∆°n nhi·ªÅu)
        
        Returns:
            Dict[column_name, column_info]
        """
        # Prepare summary for all columns
        columns_data = []
        
        for col in self.df.columns:
            col_data = self.df[col]
            sample_values = col_data.dropna().head(3).tolist()
            
            col_summary = {
                'name': col,
                'dtype': str(col_data.dtype),
                'unique': int(col_data.nunique()),
                'missing': int(col_data.isnull().sum()),
                'sample': sample_values
            }
            
            # Add stats for numeric
            if col_data.dtype in ['int64', 'float64']:
                col_summary['stats'] = {
                    'min': float(col_data.min()) if not pd.isna(col_data.min()) else None,
                    'max': float(col_data.max()) if not pd.isna(col_data.max()) else None,
                    'mean': float(col_data.mean()) if not pd.isna(col_data.mean()) else None
                }
            
            columns_data.append(col_summary)
        
        # Build batch prompt
        prompt = f"""
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch d·ªØ li·ªáu VNPT Vi·ªÖn th√¥ng. Ph√¢n t√≠ch T·∫§T C·∫¢ c√°c c·ªôt d·ªØ li·ªáu sau:

**Danh s√°ch c·ªôt**:
{json.dumps(columns_data, ensure_ascii=False, indent=2)}

**Y√äU C·∫¶U QUAN TR·ªåNG**:
Cho M·ªñI c·ªôt, b·∫°n PH·∫¢I:

1. **Ph√¢n t√≠ch t√™n c·ªôt**: 
   - N·∫øu l√† vi·∫øt t·∫Øt ‚Üí gi·∫£i th√≠ch ƒë·∫ßy ƒë·ªß (VD: "Domvi" c√≥ th·ªÉ l√† "Doanh thu vi t√≠nh" ho·∫∑c "Domain VI")
   - N·∫øu l√† ti·∫øng Anh ‚Üí d·ªãch sang ti·∫øng Vi·ªát
   - N·∫øu l√† ti·∫øng Vi·ªát ‚Üí gi·∫£i th√≠ch r√µ r√†ng

2. **Ph√¢n t√≠ch d·ªØ li·ªáu m·∫´u**:
   - Xem sample values ƒë·ªÉ hi·ªÉu c·ªôt ch·ª©a g√¨
   - Xem dtype, min/max, unique ƒë·ªÉ x√°c ƒë·ªãnh lo·∫°i d·ªØ li·ªáu

3. **ƒê∆∞a ra √Ω nghƒ©a C·ª§ TH·ªÇ**:
   - KH√îNG ƒë∆∞·ª£c ch·ªâ l·∫∑p l·∫°i t√™n c·ªôt
   - PH·∫¢I gi·∫£i th√≠ch c·ªôt n√†y ch·ª©a th√¥ng tin g√¨
   - VD: Thay v√¨ "Domvi" ‚Üí "Doanh thu vi t√≠nh" ho·∫∑c "T·ªïng doanh thu t·ª´ d·ªãch v·ª• data"

**DOMAIN KNOWLEDGE - VNPT Vi·ªÖn th√¥ng**:
- **TKC** = T√†i Kho·∫£n Ch√≠nh (s·ªë d∆∞ t√†i kho·∫£n, KH√îNG ph·∫£i "Ti·ªÅn khuy·∫øn c√°o")
- **Total_TKC** = T·ªïng s·ªë ti·ªÅn trong t√†i kho·∫£n ch√≠nh
- **PHONE/SDT** = S·ªë ƒëi·ªán tho·∫°i kh√°ch h√†ng (identifier)
- **TINH/PROVINCE** = T·ªânh/Th√†nh ph·ªë
- **NGAY_KICH_HOAT/DATE_ENTER_ACTIVE** = Ng√†y k√≠ch ho·∫°t d·ªãch v·ª•
- **ACCOUNT_AGE** = Tu·ªïi t√†i kho·∫£n (s·ªë ng√†y t·ª´ khi k√≠ch ho·∫°t)
- **SERVICE** = Lo·∫°i d·ªãch v·ª• (Data, Voice, SMS, VAS...)
- **ARPU** = Average Revenue Per User (Doanh thu trung b√¨nh/user)
- **STAFF_CODE** = M√£ nh√¢n vi√™n qu·∫£n l√Ω
- **CHURN** = R·ªùi m·∫°ng (kh√°ch h√†ng ng·ª´ng s·ª≠ d·ª•ng d·ªãch v·ª•)
- **Domvi** = C√≥ th·ªÉ l√† "Doanh thu vi t√≠nh" ho·∫∑c domain-specific term

**CATEGORY**:
- financial: Ti·ªÅn, doanh thu, chi ph√≠, TKC...
- demographic: Tu·ªïi, gi·ªõi t√≠nh, ƒë·ªãa ch·ªâ, t·ªânh...
- behavioral: H√†nh vi s·ª≠ d·ª•ng, service, churn...
- temporal: Ng√†y th√°ng, th·ªùi gian
- identifier: ID, phone, m√£ KH...
- other: Kh√¥ng thu·ªôc c√°c lo·∫°i tr√™n

**CONFIDENCE**:
- 1.0: Ch·∫Øc ch·∫Øn 100% (hardcoded terms ho·∫∑c r√µ r√†ng)
- 0.8-0.9: R·∫•t ch·∫Øc (t√™n c·ªôt + sample values kh·ªõp)
- 0.6-0.7: Kh√° ch·∫Øc (t√™n c·ªôt g·ª£i √Ω, sample values h·ª£p l√Ω)
- 0.4-0.5: Kh√¥ng ch·∫Øc (t√™n c·ªôt m∆° h·ªì, sample values kh√¥ng r√µ)
- <0.4: ƒêo√°n m√≤ (kh√¥ng ƒë·ªß th√¥ng tin)

**REASONING**:
Gi·∫£i th√≠ch T·∫†I SAO b·∫°n ƒëo√°n nh∆∞ v·∫≠y:
- D·ª±a v√†o t√™n c·ªôt (vi·∫øt t·∫Øt g√¨, nghƒ©a g√¨)
- D·ª±a v√†o sample values (gi√° tr·ªã nh∆∞ th·∫ø n√†o)
- D·ª±a v√†o dtype, stats (s·ªë, text, date...)
- D·ª±a v√†o domain knowledge VNPT

**V√ç D·ª§ T·ªêT**:
{{
    "Domvi": {{
        "meaning_vi": "Doanh thu vi t√≠nh (doanh thu t·ª´ d·ªãch v·ª• data/internet)",
        "meaning_en": "Data service revenue",
        "category": "financial",
        "confidence": 0.75,
        "reasoning": "T√™n c·ªôt 'Domvi' c√≥ th·ªÉ l√† vi·∫øt t·∫Øt c·ªßa 'Doanh thu vi t√≠nh'. Sample values l√† s·ªë, dtype l√† float64, c√≥ gi√° tr·ªã min/max/mean ‚Üí kh·∫£ nƒÉng cao l√† doanh thu. Trong ng√†nh vi·ªÖn th√¥ng, 'vi t√≠nh' th∆∞·ªùng ch·ªâ d·ªãch v·ª• data/internet."
    }}
}}

**V√ç D·ª§ X·∫§U** (KH√îNG ƒë∆∞·ª£c l√†m nh∆∞ n√†y):
{{
    "Domvi": {{
        "meaning_vi": "Domvi",  ‚Üê SAI! Ch·ªâ l·∫∑p l·∫°i t√™n c·ªôt
        "meaning_en": "Domvi",  ‚Üê SAI! Kh√¥ng gi·∫£i th√≠ch
        "category": "other",
        "confidence": 0.5,
        "reasoning": "Unknown"  ‚Üê SAI! Kh√¥ng ph√¢n t√≠ch
    }}
}}

**Tr·∫£ v·ªÅ JSON** (object v·ªõi key l√† t√™n c·ªôt):
{{
    "COLUMN_NAME_1": {{
        "meaning_vi": "√ù nghƒ©a C·ª§ TH·ªÇ, KH√îNG l·∫∑p l·∫°i t√™n c·ªôt",
        "meaning_en": "Specific English meaning",
        "category": "financial|demographic|behavioral|temporal|identifier|other",
        "confidence": 0.0-1.0,
        "reasoning": "Gi·∫£i th√≠ch chi ti·∫øt d·ª±a tr√™n t√™n c·ªôt, sample values, dtype, domain knowledge"
    }},
    ...
}}

**JSON**:
"""
        
        try:
            response = self.model.generate_content(prompt)
            
            # Extract JSON
            json_text = self._extract_json(response.text)
            results = json.loads(json_text)
            
            # Post-process: Fix common VNPT terms (hardcoded rules)
            results = self._apply_vnpt_corrections(results)
            
            # Add metadata
            for col, info in results.items():
                info['user_edited'] = False
                info['original_ai_meaning'] = info.get('meaning_vi', col)
            
            return results
            
        except Exception as e:
            print(f"Error in batch inference: {e}")
            raise
    
    def _apply_vnpt_corrections(self, results: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """
        √Åp d·ª•ng corrections cho c√°c thu·∫≠t ng·ªØ VNPT ph·ªï bi·∫øn
        Override AI n·∫øu sai
        """
        VNPT_CORRECTIONS = {
            'TKC': {
                'meaning_vi': 'T√†i kho·∫£n ch√≠nh',
                'meaning_en': 'Main account balance',
                'category': 'financial',
                'confidence': 1.0,
                'reasoning': 'Hardcoded VNPT term'
            },
            'TOTAL_TKC': {
                'meaning_vi': 'T·ªïng s·ªë ti·ªÅn trong t√†i kho·∫£n ch√≠nh',
                'meaning_en': 'Total amount in main account',
                'category': 'financial',
                'confidence': 1.0,
                'reasoning': 'Hardcoded VNPT term'
            },
            'PHONE': {
                'meaning_vi': 'S·ªë ƒëi·ªán tho·∫°i kh√°ch h√†ng',
                'meaning_en': 'Customer phone number',
                'category': 'identifier',
                'confidence': 1.0,
                'reasoning': 'Hardcoded VNPT term'
            },
            'SDT': {
                'meaning_vi': 'S·ªë ƒëi·ªán tho·∫°i',
                'meaning_en': 'Phone number',
                'category': 'identifier',
                'confidence': 1.0,
                'reasoning': 'Hardcoded VNPT term'
            }
        }
        
        # Apply corrections
        for col_name, correction in VNPT_CORRECTIONS.items():
            # Check exact match or contains
            for col in results.keys():
                if col.upper() == col_name or col_name in col.upper():
                    # Override AI inference
                    results[col] = correction.copy()
                    print(f"Applied VNPT correction for {col}: {correction['meaning_vi']}")
        
        return results
    
    def _infer_single_column(self, column: str) -> Dict[str, Any]:
        """
        ƒêo√°n √Ω nghƒ©a m·ªôt c·ªôt b·∫±ng Gemini AI
        
        Args:
            column: T√™n c·ªôt
        
        Returns:
            Dict ch·ª©a meaning, category, confidence, reasoning
        """
        # Prepare column analysis data
        col_data = self.df[column]
        dtype = str(col_data.dtype)
        sample_values = col_data.dropna().head(5).tolist()
        unique_count = col_data.nunique()
        missing_count = col_data.isnull().sum()
        
        # Statistics for numeric columns
        stats = {}
        if col_data.dtype in ['int64', 'float64']:
            stats = {
                'min': float(col_data.min()) if not pd.isna(col_data.min()) else None,
                'max': float(col_data.max()) if not pd.isna(col_data.max()) else None,
                'mean': float(col_data.mean()) if not pd.isna(col_data.mean()) else None
            }
        
        # Build prompt for Gemini
        prompt = f"""
Ph√¢n t√≠ch c·ªôt d·ªØ li·ªáu v√† ƒëo√°n √Ω nghƒ©a:

**Th√¥ng tin c·ªôt**:
- T√™n c·ªôt: {column}
- Ki·ªÉu d·ªØ li·ªáu: {dtype}
- S·ªë gi√° tr·ªã unique: {unique_count}
- S·ªë gi√° tr·ªã missing: {missing_count}
- M·∫´u d·ªØ li·ªáu: {sample_values}
{f"- Th·ªëng k√™: {stats}" if stats else ""}

**Y√™u c·∫ßu**:
D·ª±a tr√™n t√™n c·ªôt (c√≥ th·ªÉ vi·∫øt t·∫Øt), ki·ªÉu d·ªØ li·ªáu, v√† gi√° tr·ªã m·∫´u, h√£y ƒëo√°n:
1. √ù nghƒ©a c·ªßa c·ªôt (ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu)
2. √ù nghƒ©a ti·∫øng Anh
3. Danh m·ª•c d·ªØ li·ªáu (financial/demographic/behavioral/temporal/identifier/other)
4. ƒê·ªô tin c·∫≠y (0.0-1.0)
5. L√Ω do ƒëo√°n nh∆∞ v·∫≠y

**L∆∞u √Ω**:
- TKC = T√†i Kho·∫£n Ch√≠nh (ph·ªï bi·∫øn trong vi·ªÖn th√¥ng VN)
- PHONE = S·ªë ƒëi·ªán tho·∫°i
- TINH = T·ªânh/Th√†nh ph·ªë
- N·∫øu kh√¥ng ch·∫Øc, confidence th·∫•p h∆°n

**Tr·∫£ v·ªÅ JSON**:
{{
    "meaning_vi": "√ù nghƒ©a ti·∫øng Vi·ªát",
    "meaning_en": "English meaning",
    "category": "financial|demographic|behavioral|temporal|identifier|other",
    "confidence": 0.95,
    "reasoning": "L√Ω do ng·∫Øn g·ªçn"
}}

**JSON**:
"""
        
        try:
            response = self.model.generate_content(prompt)
            
            # Extract JSON from response
            json_text = self._extract_json(response.text)
            result = json.loads(json_text)
            
            # Add metadata
            result['user_edited'] = False
            result['original_ai_meaning'] = result['meaning_vi']
            
            return result
            
        except Exception as e:
            print(f"Error inferring column {column}: {e}")
            return self._basic_column_info(column)
    
    def _basic_inference(self) -> Dict[str, Dict[str, Any]]:
        """Fallback inference without AI"""
        result = {}
        for col in self.df.columns:
            result[col] = self._basic_column_info(col)
        return result
    
    def _basic_column_info(self, column: str) -> Dict[str, Any]:
        """Basic column info without AI"""
        col_data = self.df[column]
        
        # Simple category detection
        category = 'other'
        if 'phone' in column.lower() or 'tel' in column.lower():
            category = 'identifier'
        elif 'date' in column.lower() or 'time' in column.lower():
            category = 'temporal'
        elif 'price' in column.lower() or 'amount' in column.lower() or 'tkc' in column.lower():
            category = 'financial'
        elif 'name' in column.lower() or 'address' in column.lower():
            category = 'demographic'
        
        return {
            'meaning_vi': column.replace('_', ' ').title(),
            'meaning_en': column.replace('_', ' ').title(),
            'category': category,
            'confidence': 0.5,
            'reasoning': 'Fallback inference (AI not available)',
            'user_edited': False,
            'original_ai_meaning': column
        }
    
    def _extract_json(self, text: str) -> str:
        """Extract JSON from markdown or text"""
        import re
        
        # Try to find JSON in code blocks
        json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', text, re.DOTALL)
        if json_match:
            return json_match.group(1)
        
        # Try to find JSON directly
        json_match = re.search(r'\{.*\}', text, re.DOTALL)
        if json_match:
            return json_match.group(0)
        
        raise ValueError("No JSON found in response")
    
    def update_meaning(self, column: str, meaning_vi: str, meaning_en: str = None):
        """
        C·∫≠p nh·∫≠t √Ω nghƒ©a do user s·ª≠a
        
        Args:
            column: T√™n c·ªôt
            meaning_vi: √ù nghƒ©a ti·∫øng Vi·ªát
            meaning_en: √ù nghƒ©a ti·∫øng Anh (optional)
        """
        if column in self.dictionary:
            self.dictionary[column]['meaning_vi'] = meaning_vi
            if meaning_en:
                self.dictionary[column]['meaning_en'] = meaning_en
            self.dictionary[column]['user_edited'] = True
        else:
            # Create new entry
            self.dictionary[column] = {
                'meaning_vi': meaning_vi,
                'meaning_en': meaning_en or meaning_vi,
                'category': 'other',
                'confidence': 1.0,
                'reasoning': 'User defined',
                'user_edited': True,
                'original_ai_meaning': ''
            }
    
    def get_meaning(self, column: str, lang='vi') -> str:
        """
        L·∫•y √Ω nghƒ©a c·ªßa m·ªôt c·ªôt
        
        Args:
            column: T√™n c·ªôt
            lang: Ng√¥n ng·ªØ ('vi' ho·∫∑c 'en')
        
        Returns:
            str: √ù nghƒ©a c·ªßa c·ªôt
        """
        if column not in self.dictionary:
            return column
        
        key = 'meaning_vi' if lang == 'vi' else 'meaning_en'
        return self.dictionary[column].get(key, column)
    
    def get_confidence(self, column: str) -> float:
        """L·∫•y ƒë·ªô tin c·∫≠y c·ªßa AI inference"""
        if column not in self.dictionary:
            return 0.0
        return self.dictionary[column].get('confidence', 0.0)
    
    def get_category(self, column: str) -> str:
        """L·∫•y category c·ªßa c·ªôt"""
        if column not in self.dictionary:
            return 'other'
        return self.dictionary[column].get('category', 'other')
    
    def to_context_string(self) -> str:
        """
        Chuy·ªÉn dictionary th√†nh string ƒë·ªÉ d√πng l√†m context cho AI
        
        Returns:
            str: Formatted string v·ªõi √Ω nghƒ©a c√°c c·ªôt
        """
        lines = ["Column Dictionary:"]
        for col, info in self.dictionary.items():
            lines.append(f"- {col}: {info['meaning_vi']} ({info['category']})")
        return "\n".join(lines)
    
    def save_to_session(self):
        """L∆∞u v√†o Streamlit session state"""
        st.session_state.column_dictionary = self.dictionary
    
    def load_from_session(self):
        """Load t·ª´ Streamlit session state"""
        if 'column_dictionary' in st.session_state:
            self.dictionary = st.session_state.column_dictionary
    
    def export_to_json(self) -> str:
        """
        Export dictionary ra JSON string
        
        Returns:
            str: JSON string
        """
        return json.dumps(self.dictionary, ensure_ascii=False, indent=2)
    
    def import_from_json(self, json_str: str):
        """
        Import dictionary t·ª´ JSON string
        
        Args:
            json_str: JSON string
        """
        self.dictionary = json.loads(json_str)
    
    @classmethod
    def from_json(cls, json_str: str, df: pd.DataFrame, model):
        """
        T·∫°o ColumnDictionary t·ª´ JSON string
        
        Args:
            json_str: JSON string
            df: DataFrame
            model: Gemini model
        
        Returns:
            ColumnDictionary instance
        """
        instance = cls(df, model)
        instance.import_from_json(json_str)
        return instance
    
    def get_summary_stats(self) -> Dict[str, Any]:
        """L·∫•y th·ªëng k√™ t·ªïng quan v·ªÅ dictionary"""
        total_columns = len(self.dictionary)
        user_edited = sum(1 for info in self.dictionary.values() if info.get('user_edited', False))
        avg_confidence = sum(info.get('confidence', 0) for info in self.dictionary.values()) / total_columns if total_columns > 0 else 0
        
        categories = {}
        for info in self.dictionary.values():
            cat = info.get('category', 'other')
            categories[cat] = categories.get(cat, 0) + 1
        
        return {
            'total_columns': total_columns,
            'user_edited': user_edited,
            'ai_inferred': total_columns - user_edited,
            'avg_confidence': round(avg_confidence, 2),
            'categories': categories
        }


def initialize_column_dictionary(df: pd.DataFrame) -> ColumnDictionary:
    """
    Initialize ho·∫∑c load Column Dictionary t·ª´ session state
    
    Args:
        df: DataFrame
    
    Returns:
        ColumnDictionary instance
    """
    from gemini_assistant import model
    
    # Check if already exists in session
    if 'column_dict_obj' in st.session_state:
        col_dict = st.session_state.column_dict_obj
        # Update df if changed
        col_dict.df = df
        return col_dict
    
    # Create new instance
    col_dict = ColumnDictionary(df, model)
    
    # Try to load from session state
    col_dict.load_from_session()
    
    # If empty, run auto-detection
    if not col_dict.dictionary:
        with st.spinner("ü§ñ AI ƒëang ph√¢n t√≠ch √Ω nghƒ©a c√°c c·ªôt..."):
            col_dict.auto_detect_meanings()
            col_dict.save_to_session()
    
    # Save object to session
    st.session_state.column_dict_obj = col_dict
    
    return col_dict
